{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ECEfssdWYOdO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "2dUSGMhQU8Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Set the path to the main folder containing train, test, and valid folders\n",
        "data_dir = '/content/drive/MyDrive/Flower-Classification-1'\n",
        "\n",
        "batch_size = 32\n",
        "image_size = (64, 64)\n",
        "\n",
        "# Use the image_dataset_from_directory function to load the datasets\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir + '/train',\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    shuffle=True,\n",
        "    label_mode= 'categorical',\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "test_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir + '/test',\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    label_mode= 'categorical',\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir + '/valid',\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,\n",
        "    label_mode= 'categorical',\n",
        "    shuffle=False,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkOQlRImGY4C",
        "outputId": "6309aa48-29e0-4426-c56e-2e18a099f14f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3018 files belonging to 5 classes.\n",
            "Found 432 files belonging to 5 classes.\n",
            "Found 861 files belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cjO1hM18MS3i",
        "outputId": "ff9263a1-9a75-45f9-d7d4-67f2cae0fe41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the CNN model without using tf.function\n",
        "cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    tf.keras.layers.BatchNormalization(),  # Batch Normalization\n",
        "    tf.keras.layers.Dropout(0.2),  # Dropout layer\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dropout(0.5),  # Dropout layer\n",
        "    tf.keras.layers.Dense(5, activation='softmax', kernel_regularizer=tf.keras.regularizers.l2(0.01))\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Set the path to the main folder containing train, test, and valid folders\n",
        "data_dir = '/content/drive/MyDrive/Flower-Classification-1'\n",
        "\n",
        "batch_size = 32\n",
        "image_size = (224, 224)  # Set the desired input size\n",
        "\n",
        "# Use the image_dataset_from_directory function to load the datasets\n",
        "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir + '/train',\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,  # Set the desired input size\n",
        "    shuffle=True,\n",
        "    label_mode='categorical',\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "valid_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    directory=data_dir + '/valid',\n",
        "    batch_size=batch_size,\n",
        "    image_size=image_size,  # Set the desired input size\n",
        "    label_mode='categorical',\n",
        "    shuffle=False,\n",
        ")\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation and resizing\n",
        "datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Apply data augmentation and resizing to your training dataset\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir + '/train',\n",
        "    target_size=image_size,  # Set the desired input size\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Apply resizing to your validation dataset\n",
        "valid_generator = datagen.flow_from_directory(\n",
        "    data_dir + '/valid',\n",
        "    target_size=image_size,  # Set the desired input size\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Fit the model with the augmented and resized data\n",
        "history = cnn.fit(train_generator, validation_data=valid_generator, epochs=10)\n"
      ],
      "metadata": {
        "id": "Nki_Tkk0lDmf",
        "outputId": "00967743-fc08-4f88-868f-fb6ff16c50d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3018 files belonging to 5 classes.\n",
            "Found 861 files belonging to 5 classes.\n",
            "Found 3018 images belonging to 5 classes.\n",
            "Found 861 images belonging to 5 classes.\n",
            "Epoch 1/10\n",
            "95/95 [==============================] - 592s 6s/step - loss: 8.3535 - accuracy: 0.2502 - val_loss: 3.7751 - val_accuracy: 0.2207\n",
            "Epoch 2/10\n",
            "95/95 [==============================] - 93s 982ms/step - loss: 3.4756 - accuracy: 0.2356 - val_loss: 3.0486 - val_accuracy: 0.2242\n",
            "Epoch 3/10\n",
            "95/95 [==============================] - 84s 888ms/step - loss: 2.8658 - accuracy: 0.2359 - val_loss: 2.6416 - val_accuracy: 0.2242\n",
            "Epoch 4/10\n",
            "95/95 [==============================] - 93s 978ms/step - loss: 2.5262 - accuracy: 0.2449 - val_loss: 2.3773 - val_accuracy: 0.2276\n",
            "Epoch 5/10\n",
            "95/95 [==============================] - 83s 872ms/step - loss: 2.3287 - accuracy: 0.2270 - val_loss: 2.6900 - val_accuracy: 0.3113\n",
            "Epoch 6/10\n",
            "95/95 [==============================] - 84s 882ms/step - loss: 2.0632 - accuracy: 0.2518 - val_loss: 3.4535 - val_accuracy: 0.2288\n",
            "Epoch 7/10\n",
            "95/95 [==============================] - 82s 867ms/step - loss: 1.9838 - accuracy: 0.2525 - val_loss: 2.1208 - val_accuracy: 0.2253\n",
            "Epoch 8/10\n",
            "95/95 [==============================] - 83s 873ms/step - loss: 1.8327 - accuracy: 0.2462 - val_loss: 1.7888 - val_accuracy: 0.1452\n",
            "Epoch 9/10\n",
            "95/95 [==============================] - 84s 882ms/step - loss: 1.7541 - accuracy: 0.2465 - val_loss: 1.7610 - val_accuracy: 0.2276\n",
            "Epoch 10/10\n",
            "95/95 [==============================] - 83s 871ms/step - loss: 1.7650 - accuracy: 0.2462 - val_loss: 1.8144 - val_accuracy: 0.2393\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define paths\n",
        "train_dir = data_dir + '/train'\n",
        "valid_dir =data_dir + '/valid'\n",
        "# Define constants\n",
        "batch_size = 32\n",
        "img_size = (64, 64)\n",
        "\n",
        "# Create data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "valid_dataset = valid_datagen.flow_from_directory(\n",
        "    valid_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "# Build the CNN model\n",
        "cnn = models.Sequential()\n",
        "\n",
        "cnn.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)))\n",
        "cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "cnn.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "cnn.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "cnn.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "cnn.add(layers.Flatten())\n",
        "\n",
        "cnn.add(layers.Dense(128, activation='relu'))\n",
        "cnn.add(layers.Dense(5, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model with the augmented and resized data\n",
        "history = cnn.fit(train_dataset, validation_data=valid_dataset, epochs=20)\n"
      ],
      "metadata": {
        "id": "bvAAbDk3sikI",
        "outputId": "495d0a0f-4fc1-4347-987b-db0b8c5d82d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3018 images belonging to 5 classes.\n",
            "Found 861 images belonging to 5 classes.\n",
            "Epoch 1/20\n",
            "95/95 [==============================] - 21s 214ms/step - loss: 1.3280 - accuracy: 0.3989 - val_loss: 1.2347 - val_accuracy: 0.4866\n",
            "Epoch 2/20\n",
            "95/95 [==============================] - 20s 208ms/step - loss: 1.1214 - accuracy: 0.5368 - val_loss: 1.0403 - val_accuracy: 0.6051\n",
            "Epoch 3/20\n",
            "95/95 [==============================] - 19s 199ms/step - loss: 1.0034 - accuracy: 0.6004 - val_loss: 1.0929 - val_accuracy: 0.5610\n",
            "Epoch 4/20\n",
            "95/95 [==============================] - 21s 216ms/step - loss: 0.9407 - accuracy: 0.6345 - val_loss: 0.9029 - val_accuracy: 0.6481\n",
            "Epoch 5/20\n",
            "95/95 [==============================] - 22s 237ms/step - loss: 0.8676 - accuracy: 0.6547 - val_loss: 0.8950 - val_accuracy: 0.6516\n",
            "Epoch 6/20\n",
            "95/95 [==============================] - 19s 204ms/step - loss: 0.8514 - accuracy: 0.6677 - val_loss: 0.9488 - val_accuracy: 0.6272\n",
            "Epoch 7/20\n",
            "95/95 [==============================] - 20s 208ms/step - loss: 0.7844 - accuracy: 0.6962 - val_loss: 0.9007 - val_accuracy: 0.6469\n",
            "Epoch 8/20\n",
            "95/95 [==============================] - 19s 203ms/step - loss: 0.7331 - accuracy: 0.7177 - val_loss: 0.8558 - val_accuracy: 0.6748\n",
            "Epoch 9/20\n",
            "95/95 [==============================] - 20s 211ms/step - loss: 0.7014 - accuracy: 0.7286 - val_loss: 0.9019 - val_accuracy: 0.6667\n",
            "Epoch 10/20\n",
            "95/95 [==============================] - 19s 199ms/step - loss: 0.6680 - accuracy: 0.7422 - val_loss: 0.9181 - val_accuracy: 0.6852\n",
            "Epoch 11/20\n",
            "95/95 [==============================] - 19s 202ms/step - loss: 0.6698 - accuracy: 0.7469 - val_loss: 0.9052 - val_accuracy: 0.6643\n",
            "Epoch 12/20\n",
            "95/95 [==============================] - 22s 230ms/step - loss: 0.5980 - accuracy: 0.7710 - val_loss: 0.8772 - val_accuracy: 0.6899\n",
            "Epoch 13/20\n",
            "95/95 [==============================] - 19s 201ms/step - loss: 0.5619 - accuracy: 0.7816 - val_loss: 0.8265 - val_accuracy: 0.7050\n",
            "Epoch 14/20\n",
            "95/95 [==============================] - 21s 218ms/step - loss: 0.5272 - accuracy: 0.7966 - val_loss: 0.8876 - val_accuracy: 0.6864\n",
            "Epoch 15/20\n",
            "95/95 [==============================] - 20s 207ms/step - loss: 0.5126 - accuracy: 0.8019 - val_loss: 0.9556 - val_accuracy: 0.6841\n",
            "Epoch 16/20\n",
            "95/95 [==============================] - 19s 203ms/step - loss: 0.4627 - accuracy: 0.8244 - val_loss: 0.8590 - val_accuracy: 0.7143\n",
            "Epoch 17/20\n",
            "95/95 [==============================] - 21s 221ms/step - loss: 0.4428 - accuracy: 0.8264 - val_loss: 0.9687 - val_accuracy: 0.6887\n",
            "Epoch 18/20\n",
            "95/95 [==============================] - 21s 216ms/step - loss: 0.4092 - accuracy: 0.8436 - val_loss: 0.8865 - val_accuracy: 0.7154\n",
            "Epoch 19/20\n",
            "95/95 [==============================] - 20s 211ms/step - loss: 0.3693 - accuracy: 0.8628 - val_loss: 1.0963 - val_accuracy: 0.6760\n",
            "Epoch 20/20\n",
            "95/95 [==============================] - 19s 203ms/step - loss: 0.3667 - accuracy: 0.8648 - val_loss: 1.0139 - val_accuracy: 0.6829\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "cxsyuSd-p3Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn.add(tf.keras.layers.Conv2D(filters=32,\n",
        "#                               kernel_size=3,\n",
        "#                               activation='relu',\n",
        "#                               input_shape=(224,224,3)))"
      ],
      "metadata": {
        "id": "dqLB_y0sp5JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,\n",
        "#                                  strides=2))"
      ],
      "metadata": {
        "id": "jX77daflqJaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn.add(tf.keras.layers.Conv2D(filters=32,\n",
        "#                               kernel_size=3,\n",
        "#                               activation='relu'))\n",
        "# cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,\n",
        "#                                  strides=2))"
      ],
      "metadata": {
        "id": "UBSDgB_uqLWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "G0mQIkeUqO3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "\n",
        "# cnn.add(tf.keras.layers.Dense(units=5, activation='softmax'))"
      ],
      "metadata": {
        "id": "RysonszjqRau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator for data augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Apply data augmentation to your training dataset\n",
        "train_generator = datagen.flow_from_directory(\n",
        "    data_dir + '/train',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n"
      ],
      "metadata": {
        "id": "rp7lM2RjkR_f",
        "outputId": "5fe0543a-c3b8-46c1-fe3b-145955e407bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3018 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# Add Dropout layers to your model\n",
        "cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),  # Add dropout here\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),  # Add dropout here\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "r0TdxZ-BOJty"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import regularizers\n",
        "\n",
        "# Add L2 regularization to Dense layers\n",
        "cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
        "    tf.keras.layers.Dense(5, activation='softmax')\n",
        "])\n"
      ],
      "metadata": {
        "id": "dKujdtk4kl0c"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "oJvrjHCZq23W"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.fit(train_dataset, validation_data=valid_dataset, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "CNxNQ98fq46B",
        "outputId": "39ea2ad8-eaea-49b6-8d88-8a3ac65acad1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-e7300a425cb8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1377, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1360, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1349, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1126, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_8\" is incompatible with the layer: expected shape=(None, 224, 224, 3), found shape=(None, 64, 64, 3)\n"
          ]
        }
      ]
    }
  ]
}